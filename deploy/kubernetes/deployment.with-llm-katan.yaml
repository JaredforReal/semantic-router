apiVersion: apps/v1
kind: Deployment
metadata:
  name: semantic-router
  namespace: vllm-semantic-router-system
  labels:
    app: semantic-router
spec:
  replicas: 1
  selector:
    matchLabels:
      app: semantic-router
  template:
    metadata:
      labels:
        app: semantic-router
    spec:
      initContainers:
        - name: model-downloader
          image: python:3.11-slim
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              echo "Installing Hugging Face CLI..."
              pip install --no-cache-dir huggingface_hub[cli]

              echo "Downloading classifier models to persistent volume..."
              cd /app/models

              # Download category classifier model
              if [ ! -d "category_classifier_modernbert-base_model" ]; then
                echo "Downloading category classifier model..."
                huggingface-cli download LLM-Semantic-Router/category_classifier_modernbert-base_model --local-dir category_classifier_modernbert-base_model
              else
                echo "Category classifier model already exists, skipping..."
              fi

              # Download PII classifier model
              if [ ! -d "pii_classifier_modernbert-base_model" ]; then
                echo "Downloading PII classifier model..."
                huggingface-cli download LLM-Semantic-Router/pii_classifier_modernbert-base_model --local-dir pii_classifier_modernbert-base_model
              else
                echo "PII classifier model already exists, skipping..."
              fi

              # Download jailbreak classifier model
              if [ ! -d "jailbreak_classifier_modernbert-base_model" ]; then
                echo "Downloading jailbreak classifier model..."
                huggingface-cli download LLM-Semantic-Router/jailbreak_classifier_modernbert-base_model --local-dir jailbreak_classifier_modernbert-base_model
              else
                echo "Jailbreak classifier model already exists, skipping..."
              fi

              # Download PII token classifier model
              if [ ! -d "pii_classifier_modernbert-base_presidio_token_model" ]; then
                echo "Downloading PII token classifier model..."
                huggingface-cli download LLM-Semantic-Router/pii_classifier_modernbert-base_presidio_token_model --local-dir pii_classifier_modernbert-base_presidio_token_model
              else
                echo "PII token classifier model already exists, skipping..."
              fi

              # Optional: Prepare Qwen model directory for llm-katan sidecar
              # NOTE: Provide the model content under /app/models/Qwen/Qwen3-0.6B via pre-populated PV
              # or customize the following block to fetch from your internal artifact store.
              if [ ! -d "Qwen/Qwen3-0.6B" ]; then
                echo "Qwen3-0.6B directory not found. Please pre-populate /app/models/Qwen/Qwen3-0.6B in the PVC or customize init script to download it."
              fi

              echo "Model directory listing:" && ls -la /app/models/
          env:
            - name: HF_HUB_CACHE
              value: /tmp/hf_cache
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          volumeMounts:
            - name: models-volume
              mountPath: /app/models
      containers:
        - name: semantic-router
          image: ghcr.io/vllm-project/semantic-router/extproc:latest
          args: ["--secure=true"]
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          ports:
            - containerPort: 50051
              name: grpc
              protocol: TCP
            - containerPort: 9190
              name: metrics
              protocol: TCP
            - containerPort: 8080
              name: classify-api
              protocol: TCP
          env:
            - name: LD_LIBRARY_PATH
              value: "/app/lib"
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
              readOnly: true
            - name: models-volume
              mountPath: /app/models
          livenessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          resources:
            requests:
              memory: "3Gi"
              cpu: "1"
            limits:
              memory: "6Gi"
              cpu: "2"
        - name: llm-katan
          image: ghcr.io/vllm-project/semantic-router/llm-katan:latest
          imagePullPolicy: IfNotPresent
          args:
            [
              "llm-katan",
              "--model",
              "/app/models/Qwen/Qwen3-0.6B",
              "--served-model-name",
              "qwen3",
              "--host",
              "0.0.0.0",
              "--port",
              "8002",
            ]
          ports:
            - containerPort: 8002
              name: katan
              protocol: TCP
          volumeMounts:
            - name: models-volume
              mountPath: /app/models
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1"
      volumes:
        - name: config-volume
          configMap:
            name: semantic-router-config
        - name: models-volume
          persistentVolumeClaim:
            claimName: semantic-router-models
